{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./python.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert-block alert-info\">\n",
    "\n",
    "<h2> 1. Python Basics</h2>\n",
    "</div>\n",
    "\n",
    "* *Python data types and functions*\n",
    "* *List Comprehensions*\n",
    "\n",
    "<div class=\"alert-block alert-info\">\n",
    "<h2> 2. Feature Engineering with Pandas</h2>\n",
    "</div>\n",
    "\n",
    "* *Dataframes - Create, Read & Write*\n",
    "* *Transformations - Drop, Filter & transform variables*\n",
    "* *Get Data Stats*\n",
    "* *Missing Imputation*\n",
    "* *Capping and Flooring*\n",
    "* *Handling Categorical Features*\n",
    "* *Data Scaling*\n",
    "\n",
    "<div class=\"alert-block alert-info\">\n",
    "<h2> 3. Advanced Features</h2>\n",
    "</div>\n",
    "\n",
    "* *Sorting, Grouping and Merging*\n",
    "* *Iterating over rows*\n",
    "* *Advanced functions*\n",
    "* *Multi processing*\n",
    "* *Plot functions*\n",
    "* *Features Selection*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import time\n",
    "from functools import reduce\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import sklearn\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_boston,load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy.random import rand\n",
    "# import networkx as nx\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "<h2> 1. Python Basics</h2>\n",
    "</div>\n",
    "\n",
    "* *Python data types and functions*\n",
    "* *List Comprehensions*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Python data types\n",
    " - Variables: int, float, string, boolean\n",
    " - Collections: List, Dict, Set, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ints = [1,1/2, int(1.0)]                     ## int python3 1/2 will return 0.5 (float)\n",
    "print (ints)\n",
    "print ([type(i) for i in ints] , \"\\n\")\n",
    "\n",
    "floats = [1.0, 1.0/2, float(1)]\n",
    "print (floats)\n",
    "print ([type(f) for f in floats], \"\\n\")\n",
    "\n",
    "strings = [\"abcdef\", str(1), str(floats)]\n",
    "print (strings)\n",
    "print ([type(s) for s in strings], \"\\n\")\n",
    "\n",
    "new_list = [5,1,2,3,3]\n",
    "new_list.append(4)\n",
    "new_list.remove(1)\n",
    "new_list.sort()\n",
    "print (new_list , \"\\n\")\n",
    "\n",
    "# Slicing\n",
    "print (new_list[:3], new_list[3:], new_list[:-3], new_list[-3:])\n",
    "new_set = {6,7}\n",
    "new_set.add(8)\n",
    "new_set = new_set.union(set(new_list))\n",
    "print (new_set, \"\\n\")\n",
    "\n",
    "new_dict = {\"int\" : 1, \"float\" : 1.0, \"String\" : \"abc\", \"boolean\" : True, \"List\" : [1,2,3], \"Set\" : {1,2,3}, \"Dict\" : {\"a\" : 1, \"b\" : 2}}\n",
    "print (new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "# shallow copy\n",
    "d=np.mat([[2,3],[1,0]])\n",
    "print (\"d:\", d)\n",
    "e=d\n",
    "e[1,1]=5\n",
    "print (\"shallow copy:\")\n",
    "print (\"e:\", e,\"\\n\",  \"d:\", d)\n",
    "\n",
    "# deep copy\n",
    "a = [1,2,3]\n",
    "print (\"a:\",a)\n",
    "b = copy.deepcopy(a)\n",
    "b.remove(1)\n",
    "print (\"deep copy:\")\n",
    "print (\"a:\",a, \"b:\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions and Lambda Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a,b):\n",
    "    return a+b\n",
    "\n",
    "add = lambda a,b : a+b\n",
    "print (add(5,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List Comprehension\n",
    " - Short hand to transform a list using for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "print ([str(i) for i in a])\n",
    "\n",
    "scores = {\"off1\" : 0.1, \"off2\" : 0.5, \"off3\" : 0.2, \"off4\" : 0.0, \"off5\" : 0.7}\n",
    "top3_offers = [off for off, score in sorted([(o,s) for o,s in scores.items()], key = lambda x : x[1], reverse = True)][:3]\n",
    "print (top3_offers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3]\n",
    "print ([str(i) for i in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[\"abc\", \"def\"]\n",
    "[c for w in words for c in w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Map and Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# r = map(func, seq)\n",
    "a = [1,2,3,4,5,6]\n",
    "b = map(lambda x : x**2, a)\n",
    "print (list(b))\n",
    "\n",
    "a = [1,2,3,4,5,6]\n",
    "b = reduce(lambda x,y : x+y, a)\n",
    "print (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2> 2. Feature Engineering with Pandas</h2>\n",
    "</div>\n",
    "\n",
    "* *Dataframes - Create, Read & Write*\n",
    "* *Transformations - Drop, Filter & transform variables*\n",
    "* *Get Data Stats*\n",
    "* *Missing Imputation*\n",
    "* *Capping and Flooring*\n",
    "* *Handling Categorical Features*\n",
    "* *Data Scaling*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating  DataFrame from Collections\n",
    " - The inital set of baby names and birth rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names  = ['Bob','Jessica','Mary','John','Mel']\n",
    "births = [968, 155, 77, 578, 973]\n",
    "BabyDataSet = list(zip(names,births))\n",
    "print (BabyDataSet, \"\\n\")\n",
    "df1 = pd.DataFrame(data = BabyDataSet, columns=['Names', 'Births'])\n",
    "print (df1, \"\\n\")\n",
    "df2 = pd.DataFrame({\"Names\" : names, \"Births\" : births})\n",
    "df2 = df2[['Names', 'Births']]\n",
    "print (df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write DataFrame to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1.to_csv('births1880.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Location = './'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Data into DataFrames\n",
    "- header    :None if header not present in data\n",
    "- sep       :specify the delimiter\n",
    "- na_values : values to be considered as null\n",
    "- dtype     : E.g. {‘a’: np.float64, ‘b’: np.int32} \n",
    "- nrows     : number of rows to be read\n",
    "- error_bad_lines:Lines with too many fields (e.g. a csv line with too many commas) will by default cause an exception   to be raised, and no DataFrame will be returned. If False, then these “bad lines” will dropped from the DataFrame that is returned\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(Location + 'TitanicTrain.csv')\n",
    "# df = pd.read_csv(Location + 'TitanicTrain.csv', header=None, sep = ',', dtype = None, nrows=  )\n",
    "print (df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting data\n",
    "\n",
    " - loc : works on labels in the index.\n",
    " - iloc: works on the positions in the index (so it only takes integers).\n",
    " - ix  :usually tries to behave like loc but falls back to behaving like iloc if the label is not in the index.\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsetting data\n",
    "print (df.iloc[:5,:2])\n",
    "print (df.loc[:5,['PassengerId','Name']])\n",
    "print (df.ix[:5,:2])\n",
    "print (df.ix[:5,['PassengerId','Name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformations\n",
    "  - drop variables\n",
    "  - filter rows\n",
    "  - transform a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "df.drop(['Name','Pclass'],axis=1)# drop\n",
    "df[(df.Age>20) & (df.Survived==1)]# filter ....use '|' for 'or' condition\n",
    "\n",
    "# to find number of survived females\n",
    "df['Survived_Females']= list(map(lambda x,y : 1 if (x==1) and (y=='female') else 0 ,df['Survived'],df['Sex']))#transform\n",
    "print (df['Survived_Females'].value_counts())\n",
    "\n",
    "x=[1,2,3,4]\n",
    "log_transform = map(lambda x : np.log(x),x)\n",
    "print (\"log transformation:\",list(log_transform))\n",
    "\n",
    "df['ls']=df.apply(lambda x :list(x[['Name','Sex']]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading data in chunks\n",
    "- You can do operations on chunks without loading the complete data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1\n",
    "reader = pd.read_csv(Location + 'TitanicTrain.csv', usecols=['PassengerId','Survived'] ,chunksize=2)\n",
    "for chunk in reader:\n",
    "    print (chunk)\n",
    "    break\n",
    "# Method 2\n",
    "reader = pd.read_csv(Location + 'TitanicTrain.csv', iterator=True)\n",
    "print (reader.get_chunk(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Data Stats\n",
    " - Column wise missing Counts\n",
    " - Column data Types\n",
    " - min, max, mean, median, std for numeric columns\n",
    " - value counts for catagorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Column-wise missing counts: \\n\")\n",
    "print (df.isnull().sum())\n",
    "\n",
    "categorical_features = [var for var in df.columns if df[var].dtype == 'O']\n",
    "numerical_features = [var for var in df.columns if var not in categorical_features]\n",
    "print (\"\\nCategoric Features:\", categorical_features, \"\\nNumeric Features:\", numerical_features)\n",
    "print (\"\\nNumerical Feature stats:\")\n",
    "df[numerical_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Pclass.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Age.fillna(df.Age.median(), inplace = True)\n",
    "df.replace('Braund, Mr. Owen Harris', 'test', inplace = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Capping and Flooring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Fare = df.Fare.clip(5,500)\n",
    "df.Fare = df.Fare.clip(np.percentile(df.Fare, 5),np.percentile(df.Fare, 95))\n",
    "df.Fare.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Categorical Features\n",
    " - One Hot Encoding\n",
    " - Label Encoding\n",
    " - Encoding with feature stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "df = pd.concat((df, pd.get_dummies(df.Sex, prefix = \"Sex\")), axis = 1)\n",
    "# Label Encoding\n",
    "labelEncoder = preprocessing.LabelEncoder()\n",
    "df['Sex_label_encoding'] = labelEncoder.fit_transform(df.Sex)\n",
    "# Encoding with feature stats\n",
    "df['pclass_avg_fare'] = df[[\"Pclass\", 'Fare']].groupby('Pclass').transform(np.mean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Scaling\n",
    " - MinMax Scaling\n",
    " - Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "standard_scaler = preprocessing.StandardScaler()\n",
    "\n",
    "df['Fare_min_max_scaled'] = min_max_scaler.fit_transform(df.Fare)\n",
    "df['Fare_standard_scaled'] = standard_scaler.fit_transform(df.Fare)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<h2> 3. Advanced Features</h2>\n",
    "</div>\n",
    "\n",
    "* *Sorting, Grouping and Merging*\n",
    "* *Iterating over rows*\n",
    "* *Advanced functions*\n",
    "* *Multi processing*\n",
    "* *Plot functions*\n",
    "* *Features Selection*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sorting, Grouping and Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting \n",
    "Sorted = df.sort_values(['Name'], ascending=True).reset_index(drop=True)# sorts and resets the index values \n",
    "Sorted = df.sort_values(['Name', 'Sex'], ascending=True)\n",
    "\n",
    "# Grouping\n",
    "df.groupby(['Pclass'])['Fare'].mean() # this creates a series with Pclass as index and Fare as values\n",
    "df.groupby(['Pclass'])['Fare'].mean().reset_index() # creates a dataframe with Pclass and Fare as two columns\n",
    "\n",
    "df.groupby(['Pclass'])['Fare'].apply(list).reset_index()# creates list of the grouped by column\n",
    "df.groupby('Pclass')['Fare'].agg(['min', 'max'])\n",
    "\n",
    "df['rank']=df.groupby(['Sex'])['Age'].rank(ascending=True)\n",
    "df[['Name','Sex','Age','rank']].head()\n",
    "\n",
    "#Merging\n",
    "df_a = df.ix[:5,:]\n",
    "df_b = df.ix[5:10,:]\n",
    "\n",
    "df_a.append(df_b,ignore_index=True)\n",
    "pd.concat([df_a,df_b],axis=0)#axis=1 concatenates along columns\n",
    "\n",
    "#df_a.join(df_b)--> joins on index\n",
    "#df_a.merge(df_b,on=[<key>],how='inner/left/right/outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iterating over rows\n",
    " - iterrows   : iterating over rows as series\n",
    " - itertuples : iterating over rows as tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterrows\n",
    "print (\"iterrows:\\n\")\n",
    "counter=0\n",
    "for index, row in df.iterrows():\n",
    "    print ('Index:',index)\n",
    "    print ('PassengerId:',row['PassengerId'] ,\",\", 'Name:',row['Name'])\n",
    "    counter+=1\n",
    "    if counter>3:\n",
    "        break\n",
    "# itertuples: \n",
    "print ('\\nitertuples:\\n')\n",
    "counter=0\n",
    "for row in df.itertuples():\n",
    "    print ('Index:',row[0])\n",
    "    print ('PassengerId:',row.PassengerId ,\",\", 'Name:',row.Name,\",\",\"Others:\", row[5:8])\n",
    "    counter+=1\n",
    "    if counter>3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced functions\n",
    "\n",
    "- pivot,melt,shift\n",
    "- masking arrays\n",
    "- random sample-itertools,dataframe.sample\n",
    "- Moving average\n",
    "- Datetime functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot \n",
    "print (\"pivot:\\n\")\n",
    "df3 = pd.DataFrame({'date':['2000-01-03','2000-01-04','2000-01-04','2000-01-03'],'var':['A','A','B','B'],\n",
    "                   'value':[0.469112,-0.282863,1.212112,1.071804]})\n",
    "print (df3)\n",
    "print (df3.pivot(index='date', columns='var', values='value'))\n",
    "# melt\n",
    "print (\"\\n melt:\\n\")\n",
    "df4= pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'},\n",
    "                  'B': {0: 1, 1: 3, 2: 5},\n",
    "                   'C': {0: 2, 1: 4, 2: 6}})\n",
    "print (df4)\n",
    "print (pd.melt(df4, id_vars=['A'], value_vars=['B', 'C']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shift\n",
    "a = pd.DataFrame({\"x\" : [1,1,1,2,2,2] , \"y\" : [10,25,30,70,50,61]})\n",
    "print (a)\n",
    "# print a\n",
    "a['z'] =a['y'] -  a['y'].shift(1)\n",
    "# print a['z']\n",
    "mask = a['x'] != a['x'].shift(1)\n",
    "# print mask\n",
    "a['z'][mask] = 0\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Masking--Masked arrays are arrays that may have missing or invalid entries.\n",
    "\n",
    "x = np.array([1, 2, 0, -1, 1])\n",
    "mx = ma.masked_array(x, mask=[0, 0, 0, 1, 0])# make fourth entry invalid\n",
    "print (\"mx:\" ,mx)\n",
    "print (\"mx_compressed:\",ma.compressed(mx))\n",
    "print (\"mx_mean:\",mx.mean())\n",
    "\n",
    "z=ma.masked_values([1.0, 1.e20, 3.0, 4.0], 1.e20)#where all values close to 1.e20 are invalid\n",
    "print (\"z:\",z)\n",
    "\n",
    "ma.masked_where(z>2, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Selection\n",
    "\n",
    "# itertools\n",
    "from itertools import permutations\n",
    "from itertools import combinations\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "print (\"permutations:\",list(permutations(['1','2','3'])))\n",
    "print (\"combinations:\" ,list(combinations_with_replacement('12345',2)))\n",
    "\n",
    "# random sample\n",
    "df.sample(frac=0.7).head()# take out 70% sample from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rolling mean,sum\n",
    "df5 = pd.DataFrame({'A':[\"a\",\"a\",\"c\",\"c\",\"d\",\"e\"],'B': [5, 8, 2, 3,1, 4]})\n",
    "print (df5)\n",
    "print (\"rolling mean:\")\n",
    "print (df5.rolling(window=2,min_periods=1).mean())# similarly sum() can be used\n",
    "#cumulative sum\n",
    "print (\"cumulative sum:\")\n",
    "df5['no_cumulative'] = df5.groupby(['A'])['B'].apply(lambda x: x.cumsum())\n",
    "df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date time functions \n",
    "\n",
    "dt = datetime.datetime.now()\n",
    "print (\"Time right now is:\", dt)\n",
    "print (\"The datetime is a class. Hence it's type is:\", type(dt))\n",
    "print (\"Hour:\", dt.hour)\n",
    "print (\"Minute:\", dt.minute)\n",
    "print (\"Minute:\", dt.second)\n",
    "print (\"Microsecond:\", dt.microsecond)\n",
    "print (\"Year:\", dt.year)\n",
    "print (\"Date:\", dt.date())\n",
    "\n",
    "\n",
    "# Formatting\"\n",
    "print (\"Time in the ISO format for quick reading:\", dt.isoformat())\n",
    "print (\"Time in a simpler format:\", dt.strftime(\"%Y %b %d %I:%M:%S %p\"))\n",
    "\n",
    "# Parsing a datetime from a string\\n\",\n",
    "print (\"Time parsed from the ISO format:\",datetime.datetime.strptime('2016-03-03T09:21:00.737887', '%Y-%m-%dT%H:%M:%S.%f'))\n",
    "print (\"Time parsed from a simpler format:\",datetime.datetime.strptime(\"2016-03-03 23:01:02\", \"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "now = datetime.datetime.today()\n",
    "next_month = now + relativedelta(months=1)\n",
    "last_month = now - relativedelta(months=1)\n",
    "print (\"Time now:\", now)\n",
    "print (\"Time after a month:\",next_month)\n",
    "    \n",
    "# time.time() can be used to see the runtime of codes  \n",
    "\n",
    "#in pandas \n",
    "sample = pd.DataFrame({'year': [2015, 2016],\n",
    "                       'month': [2, 3],\n",
    "                       'day': [4, 5]})\n",
    "pd.to_datetime(sample)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.ix[:5,['Survived']]\n",
    "df_2 = df.ix[5:10,['Survived']]  \n",
    "df_3 = df.ix[10:15,['Survived']] \n",
    "\n",
    "def operation(column, data):\n",
    "    data[column]=data[column].map(lambda x : 'S' if x==1 else 'D')\n",
    "    return data\n",
    "\n",
    "files  =[df_1,df_2,df_3]    \n",
    "column ='Survived'\n",
    "pool   = Pool(processes = 3)\n",
    "partial_call = partial(operation, column)  \n",
    "list_data    = pool.map(partial_call, files)\n",
    "pool.close()\n",
    "pd.concat(list_data,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple plot \n",
    "t = np.arange(0., 5., 0.2)\n",
    "print (t)\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
    "plt.show()\n",
    "\n",
    "#subplots\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(t):\n",
    "    return np.exp(-t) * np.cos(2*np.pi*t)\n",
    "\n",
    "t1 = np.arange(0.0, 5.0, 0.1)\n",
    "t2 = np.arange(0.0, 5.0, 0.02)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(t2, np.cos(2*np.pi*t2), 'r--')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "- Feature selection using SelectFromModel and LassoCV\n",
    "- K means clustering\n",
    "- Forming Correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1 --> Feature selection using SelectFromModel and LassoCV\n",
    "\n",
    "# Load the boston dataset.\n",
    "boston = load_boston()\n",
    "X, y = boston['data'], boston['target']\n",
    "\n",
    "clf = LassoCV()\n",
    "\n",
    "# Set a minimum threshold of 0.25\n",
    "sfm = SelectFromModel(clf, threshold=0.25)\n",
    "sfm.fit(X, y)\n",
    "n_features = sfm.transform(X).shape[1]\n",
    "\n",
    "# Reset the threshold till the number of features equals two.\n",
    "while n_features > 2:\n",
    "    sfm.threshold += 0.1\n",
    "    X_transform = sfm.transform(X)\n",
    "    n_features = X_transform.shape[1]\n",
    "\n",
    "# Plot the selected two features from X.\n",
    "plt.title(\n",
    "    \"Features selected from Boston using SelectFromModel with \"\n",
    "    \"threshold %0.3f.\" % sfm.threshold)\n",
    "feature1 = X_transform[:, 0]\n",
    "feature2 = X_transform[:, 1]\n",
    "plt.plot(feature1, feature2, 'r.')\n",
    "plt.xlabel(\"Feature number 1\")\n",
    "plt.ylabel(\"Feature number 2\")\n",
    "plt.ylim([np.min(feature2), np.max(feature2)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means clustering\n",
    "data=pd.read_csv('./test.csv')\n",
    "data=data.T\n",
    "print (data.head())\n",
    "k=2\n",
    "# we take a transpose so as to have variables as rows\n",
    "X=np.matrix(data)\n",
    "print (X.shape)\n",
    "kmeans = KMeans(n_clusters=k).fit(X)\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "color=['o','ro']\n",
    "\n",
    "for c,i in zip(color,range(k)):\n",
    "# # select only data observations with cluster label == i\n",
    "    ds = X[np.where(labels==i)]\n",
    "    plt.plot(ds[:,0],ds[:,1],c)\n",
    "    # plot the centroids\n",
    "    lines = plt.plot(centroids[i,0],centroids[i,1],'kx')\n",
    "    # make the centroid x's bigger\n",
    "    plt.setp(lines,ms=15.0)\n",
    "    plt.setp(lines,mew=2.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3 --> Forming Correlation Matrix\n",
    "\n",
    "boston   = load_boston()\n",
    "X_boston = boston.data\n",
    "y_boston  = boston.target\n",
    "\n",
    "boston=pd.DataFrame(X_boston,columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\\\n",
    "                                  'TAX', 'PTRATIO', 'B', 'LSTAT'])\n",
    "\n",
    "corr_matrix=boston.corr()\n",
    "\n",
    "g = nx.Graph()\n",
    "for var1 in boston.columns:\n",
    "    for var2 in boston.columns:\n",
    "        if var1 != var2 and corr_matrix[var1][var2] > 0.6:\n",
    "            g.add_edge(var1, var2)\n",
    "            \n",
    "            \n",
    "print ([g1.edges() for g1 in nx.connected_component_subgraphs(g)])\n",
    "\n",
    "nx.draw(g, cmap = plt.get_cmap('jet'), node_color = ['r','g','c','y'],with_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
